{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07bb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from timeit import default_timer\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "from einops import rearrange\n",
    "from wavebench.nn.fno import FNO2d\n",
    "from wavebench.nn.lploss import LpLoss\n",
    "\n",
    "\n",
    "# reading data\n",
    "class MatReader(object):\n",
    "  def __init__(self, file_path, to_torch=True, to_cuda=False, to_float=True):\n",
    "    super(MatReader, self).__init__()\n",
    "\n",
    "    self.to_torch = to_torch\n",
    "    self.to_cuda = to_cuda\n",
    "    self.to_float = to_float\n",
    "\n",
    "    self.file_path = file_path\n",
    "\n",
    "    self.data = None\n",
    "    self.old_mat = None\n",
    "    self._load_file()\n",
    "\n",
    "  def _load_file(self):\n",
    "    self.data = scipy.io.loadmat(self.file_path)\n",
    "    self.old_mat = True\n",
    "\n",
    "  def load_file(self, file_path):\n",
    "    self.file_path = file_path\n",
    "    self._load_file()\n",
    "\n",
    "  def read_field(self, field):\n",
    "    x = self.data[field]\n",
    "    if not self.old_mat:\n",
    "      x = x[()]\n",
    "      x = np.transpose(x, axes=range(len(x.shape) - 1, -1, -1))\n",
    "\n",
    "    if self.to_float:\n",
    "      x = x.astype(np.float32)\n",
    "\n",
    "    if self.to_torch:\n",
    "      x = torch.from_numpy(x)\n",
    "\n",
    "      if self.to_cuda:\n",
    "        x = x.cuda()\n",
    "\n",
    "    return x\n",
    "\n",
    "  def set_cuda(self, to_cuda):\n",
    "    self.to_cuda = to_cuda\n",
    "\n",
    "  def set_torch(self, to_torch):\n",
    "    self.to_torch = to_torch\n",
    "\n",
    "  def set_float(self, to_float):\n",
    "    self.to_float = to_float\n",
    "\n",
    "# normalization, pointwise gaussian\n",
    "class UnitGaussianNormalizer(object):\n",
    "  def __init__(self, x, eps=0.00001):\n",
    "    super(UnitGaussianNormalizer, self).__init__()\n",
    "\n",
    "    # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
    "    self.mean = torch.mean(x, 0)\n",
    "    self.std = torch.std(x, 0)\n",
    "    self.eps = eps\n",
    "\n",
    "  def encode(self, x):\n",
    "    x = (x - self.mean) / (self.std + self.eps)\n",
    "    return x\n",
    "\n",
    "  def decode(self, x, sample_idx=None):\n",
    "    if sample_idx is None:\n",
    "      std = self.std + self.eps # n\n",
    "      mean = self.mean\n",
    "    else:\n",
    "      if len(self.mean.shape) == len(sample_idx[0].shape):\n",
    "        std = self.std[sample_idx] + self.eps  # batch*n\n",
    "        mean = self.mean[sample_idx]\n",
    "      if len(self.mean.shape) > len(sample_idx[0].shape):\n",
    "        std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
    "        mean = self.mean[:,sample_idx]\n",
    "\n",
    "    # x is in shape of batch*n or T*batch*n\n",
    "    x = (x * std.to(x.device)) + mean.to(x.device)\n",
    "    return x\n",
    "\n",
    "  def cuda(self, device = torch.device('cuda:0')):\n",
    "    self.mean = self.mean.to(device)\n",
    "    self.std = self.std.to(device)\n",
    "\n",
    "  def cpu(self):\n",
    "    self.mean = self.mean.cpu()\n",
    "    self.std = self.std.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7038b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "\n",
    "config = ml_collections.ConfigDict()\n",
    "config.ntrain = 1000\n",
    "config.ntest = 100\n",
    "config.batch_size = 20\n",
    "\n",
    "config.fno_modes = 12\n",
    "config.fno_width = 32\n",
    "config.learning_rate = 0.001\n",
    "config.num_epochs = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ad3b299",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Download the darcy flow data from the Google Drive [here](https://drive.google.com/drive/folders/1UnbQh2WWc6knEHbLn-ZaXrKUZhp7pjt-).\n",
    "\n",
    "See also https://github.com/neuraloperator/neuraloperator/issues/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808203bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/liu0003/Desktop/datasets'\n",
    "TRAIN_PATH = dataset_dir + '/pde_data/Darcy_421' + '/piececonst_r421_N1024_smooth1.mat'\n",
    "TEST_PATH = dataset_dir + '/pde_data/Darcy_421' +  '/piececonst_r421_N1024_smooth2.mat'\n",
    "\n",
    "\n",
    "ntrain = config.ntrain\n",
    "ntest = config.ntest\n",
    "\n",
    "r = 5\n",
    "h = int(((421 - 1)/r) + 1)\n",
    "s = h\n",
    "\n",
    "reader = MatReader(TRAIN_PATH)\n",
    "x_train = reader.read_field('coeff')[:ntrain,::r,::r][:,:s,:s]\n",
    "y_train = reader.read_field('sol')[:ntrain,::r,::r][:,:s,:s]\n",
    "\n",
    "reader.load_file(TEST_PATH)\n",
    "x_test = reader.read_field('coeff')[:ntest,::r,::r][:,:s,:s]\n",
    "y_test = reader.read_field('sol')[:ntest,::r,::r][:,:s,:s]\n",
    "\n",
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "x_train = x_train.reshape(ntrain,s,s,1)\n",
    "x_test = x_test.reshape(ntest,s,s,1)\n",
    "\n",
    "# batch_size = 20\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torch.utils.data.TensorDataset(x_train, y_train),\n",
    "  batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torch.utils.data.TensorDataset(x_test, y_test),\n",
    "  batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67029cbe",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9ddb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.870572882995475 0.11266509556770325 0.06562536358833312\n",
      "1 1.6448596470290795 0.04840609937906265 0.03930075824260712\n",
      "2 1.6392666759784333 0.03413216185569763 0.036280732750892636\n",
      "3 1.651758732041344 0.028835083723068238 0.02738577425479889\n",
      "4 1.6398576999781653 0.02435221701860428 0.026867963075637817\n",
      "5 1.6487251510261558 0.021698871195316313 0.022876895368099212\n",
      "6 1.6583535440149717 0.02101210716366768 0.02220281183719635\n",
      "7 1.6562338539515622 0.017889699578285217 0.02010775238275528\n",
      "8 1.6718751059961505 0.018747543662786484 0.022063536942005156\n",
      "9 1.6607170030474663 0.01778507339954376 0.020209083259105684\n",
      "10 1.6657434810185805 0.01498659098148346 0.017445169389247894\n",
      "11 1.6649010769906454 0.013804593101143837 0.017885723114013673\n",
      "12 1.6676555869635195 0.012834921434521675 0.016397310495376585\n",
      "13 1.6696301529882476 0.012135858342051506 0.01565384805202484\n",
      "14 1.6715576499700546 0.013415244534611702 0.01781418889760971\n",
      "15 1.664296688977629 0.011317055001854896 0.015269584655761718\n",
      "16 1.6580626259674318 0.011044602632522584 0.016937735974788665\n",
      "17 1.6606397120049223 0.011154873967170716 0.015492411553859711\n",
      "18 1.660930771031417 0.01037124665081501 0.014533617943525314\n",
      "19 1.6628660339629278 0.010601168021559715 0.01514748364686966\n",
      "20 1.6463156800018623 0.01122012859582901 0.015061886608600616\n",
      "21 1.6607229619985446 0.01032084558904171 0.014924429655075074\n",
      "22 1.6620537319686264 0.011297605380415916 0.015687069594860076\n",
      "23 1.6692064160015434 0.011315841883420944 0.01654443293809891\n",
      "24 1.6636518260347657 0.010439097449183464 0.013481758832931519\n",
      "25 1.6607795599848032 0.00964902402460575 0.013715488910675049\n",
      "26 1.6654690960422158 0.008739992186427117 0.014006594717502594\n",
      "27 1.6645290230517276 0.00813719031214714 0.01285348728299141\n",
      "28 1.662809640984051 0.0082177132666111 0.012681461870670319\n",
      "29 1.6654119319864549 0.009586942225694657 0.014738340973854064\n",
      "30 1.66501870600041 0.008661221861839295 0.013590006083250046\n",
      "31 1.6547474559629336 0.008379932329058646 0.012072034478187561\n",
      "32 1.663926339999307 0.007696606084704399 0.01178349256515503\n",
      "33 1.6578325019800104 0.007934533566236496 0.012350186258554458\n",
      "34 1.660171130031813 0.009359724253416062 0.014226432740688324\n",
      "35 1.6672067039762624 0.008711627826094628 0.012803415954113006\n",
      "36 1.6617620519828051 0.007645893886685372 0.012502985894680024\n",
      "37 1.6623044739826582 0.0070720289126038555 0.011997999995946885\n",
      "38 1.6655856170109473 0.007002992942929268 0.013047489672899246\n",
      "39 1.668888551008422 0.00837136371433735 0.012302698194980621\n",
      "40 1.6632475659716874 0.008318415716290474 0.013047828078269959\n",
      "41 1.6644736379967071 0.007950165063142776 0.01206249326467514\n",
      "42 1.6711285240016878 0.007573500938713551 0.012241701036691666\n",
      "43 1.6677129219751805 0.007633848175406456 0.012271040081977845\n",
      "44 1.6681142440065742 0.007896237090229989 0.012851729691028595\n",
      "45 1.665229750040453 0.0070919435769319535 0.01245808333158493\n",
      "46 1.6701088580302894 0.00641991475969553 0.012028016149997711\n",
      "47 1.6728748920140788 0.0072504856958985325 0.012341282218694688\n",
      "48 1.6750957560143434 0.006720035523176193 0.01134061649441719\n",
      "49 1.6698818929726258 0.006442200243473053 0.01141913264989853\n",
      "50 1.680561525048688 0.007169953294098377 0.014021292924880982\n",
      "51 1.6728629890130833 0.008351199559867382 0.012816414535045624\n",
      "52 1.6746897849952802 0.006322252653539181 0.011012273877859115\n",
      "53 1.673437454039231 0.005723679259419441 0.011343758106231689\n",
      "54 1.6773752510198392 0.006453822150826454 0.0113598695397377\n",
      "55 1.6722918919986114 0.006085814379155636 0.011678252071142196\n",
      "56 1.6803625579923391 0.006034951508045196 0.011715841293334962\n",
      "57 1.6859162889886647 0.0071171997785568234 0.012290441691875457\n",
      "58 1.660138017963618 0.007181271098554134 0.011936269551515579\n",
      "59 1.6597978649660945 0.007930163331329822 0.011581592857837678\n",
      "60 1.6623121249722317 0.006175723657011986 0.011800906211137772\n",
      "61 1.6606187249999493 0.00669094754755497 0.011632082611322403\n",
      "62 1.667366263049189 0.005655147545039654 0.011310528367757797\n",
      "63 1.6737889630021527 0.005875453628599644 0.011447844803333282\n",
      "64 1.6746172859566286 0.006620422832667828 0.011408550441265106\n",
      "65 1.6731048679794185 0.005914642348885536 0.010827594697475433\n",
      "66 1.672197756008245 0.007054179400205612 0.011780552119016647\n",
      "67 1.664649402955547 0.006130666047334671 0.010964103490114212\n",
      "68 1.6624602610245347 0.005707374043762684 0.010968525558710099\n",
      "69 1.6719305430306122 0.00596995995938778 0.011793540269136428\n",
      "70 1.67291851202026 0.006116486705839634 0.0108131043612957\n",
      "71 1.6637728250352666 0.005346198938786984 0.010809151530265808\n",
      "72 1.6585422930074856 0.005548830009996891 0.010673431605100632\n",
      "73 1.6624740830156952 0.005276918828487396 0.010605421513319016\n",
      "74 1.6589372769813053 0.00522444199025631 0.010778630822896958\n",
      "75 1.6629137819982134 0.004750126659870148 0.01077788308262825\n",
      "76 1.664866516017355 0.005088988773524762 0.010661300271749496\n",
      "77 1.6677546229911968 0.004825691364705563 0.010791893303394317\n",
      "78 1.6661059529869817 0.004883387796580791 0.010592356324195862\n",
      "79 1.6628594690118916 0.004927717156708241 0.010797167122364044\n",
      "80 1.6690597709966823 0.005115671955049038 0.0111381496489048\n",
      "81 1.6705505950376391 0.004857296049594879 0.010832335352897644\n",
      "82 1.6602185139781795 0.00544827601313591 0.011164561808109284\n",
      "83 1.665235654974822 0.005907019272446632 0.010375544279813766\n",
      "84 1.6723982719704509 0.004666895657777786 0.010757724046707154\n",
      "85 1.678261280991137 0.005031947068870068 0.01035796046257019\n",
      "86 1.6763399980263785 0.004521432563662529 0.010784716606140136\n",
      "87 1.68603094300488 0.004705442123115063 0.010366237610578538\n",
      "88 1.6818732360261492 0.0041485391482710835 0.010821576565504074\n",
      "89 1.683368295955006 0.005058490194380283 0.01080341562628746\n",
      "90 1.6837628160137683 0.004644801124930382 0.010310960859060287\n",
      "91 1.6842458759783767 0.004171040639281273 0.010246902704238892\n",
      "92 1.672563287022058 0.0041095979735255245 0.010327009707689285\n",
      "93 1.6729465799871832 0.004228543236851692 0.01020449623465538\n",
      "94 1.6793164089904167 0.004166032262146473 0.01053395375609398\n",
      "95 1.6721608900115825 0.0043989489004015924 0.010367873609066009\n",
      "96 1.6770737600163557 0.004716874524950981 0.010165407061576844\n",
      "97 1.6799401260213926 0.004123931869864463 0.010337283015251159\n",
      "98 1.6727998559945263 0.004357067629694939 0.010637535750865935\n",
      "99 1.6721291519934312 0.004126280203461647 0.010074795335531234\n",
      "100 1.6735218210378662 0.0038061335012316705 0.01016366645693779\n",
      "101 1.6676438809954561 0.004784855931997299 0.01055098459124565\n",
      "102 1.6714879799983464 0.003953805737197399 0.010137317329645157\n",
      "103 1.6744811710086651 0.003791146546602249 0.009950307309627532\n",
      "104 1.6710865819477476 0.003495937693864107 0.010094241499900817\n",
      "105 1.6724271660204977 0.003694747567176819 0.010014019310474395\n",
      "106 1.6726460959762335 0.004306362263858318 0.010368862897157669\n",
      "107 1.6824844210059382 0.003812467090785503 0.010135903656482696\n",
      "108 1.6806884359684773 0.00375342433154583 0.010092364102602005\n",
      "109 1.6758962420281023 0.0038650975897908213 0.010091683864593505\n",
      "110 1.6838656220352277 0.003501729987561703 0.010014768689870834\n",
      "111 1.675871220009867 0.003521237164735794 0.010113262683153152\n",
      "112 1.6804022249998525 0.0036554317101836205 0.009946673065423966\n",
      "113 1.6880843130056746 0.004153846345841885 0.01003888726234436\n",
      "114 1.687643901037518 0.0037938805148005485 0.01011772319674492\n",
      "115 1.6818349129753187 0.003421148467808962 0.009807418882846832\n",
      "116 1.690493531001266 0.003019950158894062 0.009925629198551177\n",
      "117 1.6835787349846214 0.0030266801826655866 0.00993134617805481\n",
      "118 1.681985124014318 0.003153877221047878 0.00980518952012062\n",
      "119 1.684526180033572 0.0031579038128256797 0.009934049099683762\n",
      "120 1.6856124880141579 0.0031227869912981987 0.009724207818508149\n",
      "121 1.6820363840088248 0.0030968589298427103 0.009745022058486938\n",
      "122 1.68576669000322 0.0030262704342603683 0.00972795307636261\n",
      "123 1.6880815689801238 0.002910548094660044 0.009758559912443161\n",
      "124 1.6838521200115792 0.0030297803655266763 0.009741770029067993\n",
      "125 1.682751678978093 0.0030724043548107146 0.009737859666347503\n",
      "126 1.6827414539875463 0.0032313749864697457 0.009988241493701935\n",
      "127 1.6838099099695683 0.002992567203938961 0.009818952977657319\n",
      "128 1.6859964029863477 0.00297227194160223 0.0097700035572052\n",
      "129 1.6912609889986925 0.0030430688336491583 0.009722756743431092\n",
      "130 1.6867528260336258 0.0029678172320127486 0.009878333508968353\n",
      "131 1.6880080580012873 0.0027841939330101014 0.00967736765742302\n",
      "132 1.6846577569958754 0.002678658902645111 0.00969132587313652\n",
      "133 1.6869406389887445 0.0026386328004300596 0.009616990685462951\n",
      "134 1.6890227519907057 0.0025535549633204937 0.009619617164134979\n",
      "135 1.6943418069859035 0.0025257350727915765 0.0096519835293293\n",
      "136 1.6959941830136813 0.002610443640500307 0.009628513604402542\n",
      "137 1.697238461987581 0.002563780952244997 0.009640623778104783\n",
      "138 1.6984477519872598 0.002546048171818256 0.00960120901465416\n",
      "139 1.6974437320022844 0.0025404763743281365 0.009674263149499894\n",
      "140 1.6811985580134206 0.0025258803702890875 0.00960881143808365\n",
      "141 1.6894812069949694 0.0025120200030505657 0.009648136049509048\n",
      "142 1.684233726002276 0.002457275688648224 0.009631629884243011\n",
      "143 1.7030271270195954 0.002380522634834051 0.00954000160098076\n",
      "144 1.6957537879934534 0.002403545495122671 0.009620191752910614\n",
      "145 1.690210209984798 0.002401908278465271 0.00961911141872406\n",
      "146 1.7045517190126702 0.0023488856479525564 0.009570152163505555\n",
      "147 1.6957696969620883 0.0023082122169435022 0.00956687554717064\n",
      "148 1.6940508070401847 0.0023430774845182895 0.00958421766757965\n",
      "149 1.695629463007208 0.0023005850724875927 0.009568228423595428\n",
      "150 1.699993305024691 0.002282748408615589 0.00954495444893837\n",
      "151 1.7028657150221989 0.0022546537928283213 0.00954311117529869\n",
      "152 1.6838771760230884 0.0022514577731490137 0.009533670246601105\n",
      "153 1.6899098710273392 0.0022613876089453695 0.00952949121594429\n",
      "154 1.6899162220070139 0.002261109869927168 0.009563648700714111\n",
      "155 1.684089055052027 0.00223684736341238 0.00951354369521141\n",
      "156 1.6883925879956223 0.0022356687299907208 0.009543027281761169\n",
      "157 1.6880581669975072 0.0022539601884782315 0.009533011615276336\n",
      "158 1.6897409690427594 0.0022140574045479296 0.009511246532201766\n",
      "159 1.6815943499677815 0.002230001259595156 0.009518297165632248\n",
      "160 1.6952685109572485 0.0022122756503522396 0.009498618692159653\n",
      "161 1.688861136964988 0.002184996761381626 0.00950759470462799\n",
      "162 1.6901478620129637 0.0021800529062747957 0.009511207491159438\n",
      "163 1.6943834720295854 0.0021766618266701697 0.009501844495534897\n",
      "164 1.6900744040030986 0.002173400491476059 0.009500173926353455\n",
      "165 1.687341501004994 0.0021653145514428616 0.009492773562669754\n",
      "166 1.6908630290417932 0.002159995332360268 0.009495212733745574\n",
      "167 1.6848880999605171 0.002156481310725212 0.009508916288614272\n",
      "168 1.691358632990159 0.002152209993451834 0.009483471363782883\n",
      "169 1.687185494985897 0.002149039715528488 0.009501679837703704\n",
      "170 1.6843325800145976 0.002145490735769272 0.00949414387345314\n",
      "171 1.6818759770249017 0.0021403943933546543 0.00948229044675827\n",
      "172 1.6941437509958632 0.0021386377327144144 0.009481152892112732\n",
      "173 1.6939567159861326 0.002134373690932989 0.009482354670763016\n",
      "174 1.6927976140286773 0.0021324327848851682 0.009482600390911103\n",
      "175 1.685485465975944 0.0021303091756999493 0.009476273208856583\n",
      "176 1.6897240430116653 0.00212810979411006 0.009480976462364198\n",
      "177 1.6921957319718786 0.0021252123191952705 0.009479042291641235\n",
      "178 1.6891202499973588 0.0021236000321805475 0.009479284882545472\n",
      "179 1.6905452159699053 0.002120848547667265 0.009478841572999955\n",
      "180 1.6872660960070789 0.002118578311055899 0.009478234946727752\n",
      "181 1.6899937040288933 0.002116934448480606 0.009477483779191971\n",
      "182 1.6867713500396349 0.0021150583066046236 0.009467360079288483\n",
      "183 1.6861028790008277 0.0021136914305388927 0.009473196864128113\n",
      "184 1.6975295530282892 0.0021124856658279894 0.009470825791358948\n",
      "185 1.6845243719872087 0.0021112212873995303 0.009471794366836548\n",
      "186 1.6913851259741932 0.0021097838133573533 0.009467527717351914\n",
      "187 1.6872139950282872 0.0021083486154675484 0.009472734332084655\n",
      "188 1.6946210149908438 0.0021074449345469476 0.00947050854563713\n",
      "189 1.6936039270367473 0.002106432467699051 0.009471236616373063\n",
      "190 1.697487130004447 0.0021055805943906306 0.009473646134138107\n",
      "191 1.6996504929848015 0.0021046161912381648 0.009472037851810455\n",
      "192 1.7035753860254772 0.00210405657812953 0.009469403624534608\n",
      "193 1.6979805980226956 0.00210351075977087 0.009469503164291381\n",
      "194 1.7017854640143923 0.0021029299013316633 0.009470101445913315\n",
      "195 1.6967916779685766 0.0021025174148380757 0.009471859782934189\n",
      "196 1.6958145799580961 0.002102184679359198 0.009469667971134186\n",
      "197 1.6951256900210865 0.0021019511185586454 0.009468746334314347\n",
      "198 1.6911202979972586 0.002101764872670174 0.009469300508499146\n",
      "199 1.6862401320249774 0.0021016646213829517 0.009469981193542481\n"
     ]
    }
   ],
   "source": [
    "model = FNO2d(\n",
    "  modes1=config.fno_modes,\n",
    "  modes2=config.fno_modes,\n",
    "  hidden_width=config.fno_width,\n",
    "  ).cuda()\n",
    "\n",
    "# epochs = 500\n",
    "# iterations = epochs*(ntrain//batch_size)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.learning_rate\n",
    "    )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.num_epochs*len(train_loader),\n",
    "    eta_min=1e-6)\n",
    "\n",
    "myloss = LpLoss(reduction='sum')\n",
    "y_normalizer.cuda()\n",
    "for ep in range(config.num_epochs):\n",
    "  model.train()\n",
    "  t1 = default_timer()\n",
    "  train_l2 = 0\n",
    "  for x, y in train_loader:\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(rearrange(x, 'b h w c -> b c h w')).reshape(\n",
    "      config.batch_size, s, s)\n",
    "    # out = model(x).reshape(batch_size, s, s)\n",
    "\n",
    "    out = y_normalizer.decode(out)\n",
    "    y = y_normalizer.decode(y)\n",
    "\n",
    "    loss = myloss(out.view(config.batch_size,-1), y.view(config.batch_size,-1))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    train_l2 += loss.item()\n",
    "\n",
    "  model.eval()\n",
    "  test_l2 = 0.0\n",
    "  with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "      x, y = x.cuda(), y.cuda()\n",
    "\n",
    "      out = model(rearrange(x, 'b h w c -> b c h w')).reshape(\n",
    "        config.batch_size, s, s)\n",
    "      # out = model(x).reshape(batch_size, s, s)\n",
    "      out = y_normalizer.decode(out)\n",
    "\n",
    "      test_l2 += myloss(\n",
    "        out.view(config.batch_size,-1), y.view(config.batch_size,-1)).item()\n",
    "\n",
    "  train_l2/= ntrain\n",
    "  test_l2 /= ntest\n",
    "\n",
    "  t2 = default_timer()\n",
    "  print(ep, t2-t1, train_l2, test_l2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16040e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
