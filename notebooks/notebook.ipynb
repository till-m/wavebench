{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops10select_int4callERKNS_6TensorEll\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from neuralop.models import FNO2d\n",
    "\n",
    "# operator = FNO(n_modes=(16, 16), hidden_channels=64,\n",
    "#                 in_channels=3, out_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neuralop.models.tfno.FNO2d"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FNO2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FNO2d' from 'neuralop.models.tfno' (/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/tfno.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mneuralop\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m tfno\n",
      "File \u001b[0;32m~/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0.1.1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m TFNO3d, TFNO2d, TFNO1d, TFNO\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n",
      "File \u001b[0;32m~/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtfno\u001b[39;00m \u001b[39mimport\u001b[39;00m TFNO3d, TFNO2d, TFNO1d, TFNO, FNO2d\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_dispatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FNO2d' from 'neuralop.models.tfno' (/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/tfno.py)"
     ]
    }
   ],
   "source": [
    "from neuralop.models import tfno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FNO2d' from 'neuralop.models.tfno' (/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/tfno.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mneuralop\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m FNO2d\n",
      "File \u001b[0;32m~/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0.1.1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m TFNO3d, TFNO2d, TFNO1d, TFNO, FNO2d\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n",
      "File \u001b[0;32m~/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtfno\u001b[39;00m \u001b[39mimport\u001b[39;00m TFNO3d, TFNO2d, TFNO1d, TFNO, FNO2d\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel_dispatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m get_model\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FNO2d' from 'neuralop.models.tfno' (/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/tfno.py)"
     ]
    }
   ],
   "source": [
    "from neuralop.models import FNO2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FNO' from 'neuralop.models' (/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mneuralop\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m FNO\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FNO' from 'neuralop.models' (/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from neuralop.models import FNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'neuralop.models.fno_block' from '/home/liu0003/anaconda3/envs/wavebench/lib/python3.9/site-packages/neuralop/models/fno_block.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partialmethod\n",
    "import torch\n",
    "from .spectral_convolution import FactorizedSpectralConv\n",
    "from .padding import DomainPadding\n",
    "from .fno_block import FNOBlocks, resample\n",
    "\n",
    "\n",
    "class Lifting(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_dim=2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        Conv = getattr(nn, f'Conv{n_dim}d')\n",
    "        self.fc = Conv(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=None, n_dim=2, non_linearity=F.gelu):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.hidden_channels = in_channels if hidden_channels is None else hidden_channels \n",
    "        self.non_linearity = non_linearity\n",
    "        Conv = getattr(nn, f'Conv{n_dim}d')\n",
    "        self.fc1 = Conv(in_channels, hidden_channels, 1)\n",
    "        self.fc2 = Conv(hidden_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.non_linearity(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class FNO(nn.Module):\n",
    "    \"\"\"N-Dimensional Fourier Neural Operator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_modes : int tuple\n",
    "        number of modes to keep in Fourier Layer, along each dimension\n",
    "        The dimensionality of the TFNO is inferred from ``len(n_modes)``\n",
    "    hidden_channels : int\n",
    "        width of the FNO (i.e. number of channels)\n",
    "    in_channels : int, optional\n",
    "        Number of input channels, by default 3\n",
    "    out_channels : int, optional\n",
    "        Number of output channels, by default 1\n",
    "    lifting_channels : int, optional\n",
    "        number of hidden channels of the lifting block of the FNO, by default 256\n",
    "    projection_channels : int, optional\n",
    "        number of hidden channels of the projection block of the FNO, by default 256\n",
    "    n_layers : int, optional\n",
    "        Number of Fourier Layers, by default 4\n",
    "    incremental_n_modes : None or int tuple, default is None\n",
    "        * If not None, this allows to incrementally increase the number of modes in Fourier domain \n",
    "          during training. Has to verify n <= N for (n, m) in zip(incremental_n_modes, n_modes).\n",
    "        \n",
    "        * If None, all the n_modes are used.\n",
    "\n",
    "        This can be updated dynamically during training.\n",
    "    use_mlp : bool, optional\n",
    "        Whether to use an MLP layer after each FNO block, by default False\n",
    "    mlp : dict, optional\n",
    "        Parameters of the MLP, by default None\n",
    "        {'expansion': float, 'dropout': float}\n",
    "    non_linearity : nn.Module, optional\n",
    "        Non-Linearity module to use, by default F.gelu\n",
    "    norm : F.module, optional\n",
    "        Normalization layer to use, by default None\n",
    "    preactivation : bool, default is False\n",
    "        if True, use resnet-style preactivation\n",
    "    skip : {'linear', 'identity', 'soft-gating'}, optional\n",
    "        Type of skip connection to use, by default 'soft-gating'\n",
    "    separable : bool, default is False\n",
    "        if True, use a depthwise separable spectral convolution\n",
    "    factorization : str or None, {'tucker', 'cp', 'tt'}\n",
    "        Tensor factorization of the parameters weight to use, by default None.\n",
    "        * If None, a dense tensor parametrizes the Spectral convolutions\n",
    "        * Otherwise, the specified tensor factorization is used.\n",
    "    joint_factorization : bool, optional\n",
    "        Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer), by default False\n",
    "    rank : float or rank, optional\n",
    "        Rank of the tensor factorization of the Fourier weights, by default 1.0\n",
    "    fixed_rank_modes : bool, optional\n",
    "        Modes to not factorize, by default False\n",
    "    implementation : {'factorized', 'reconstructed'}, optional, default is 'factorized'\n",
    "        If factorization is not None, forward mode to use::\n",
    "        * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass\n",
    "        * `factorized` : the input is directly contracted with the factors of the decomposition\n",
    "    decomposition_kwargs : dict, optional, default is {}\n",
    "        Optionaly additional parameters to pass to the tensor decomposition\n",
    "    domain_padding : None or float, optional\n",
    "        If not None, percentage of padding to use, by default None\n",
    "    domain_padding_mode : {'symmetric', 'one-sided'}, optional\n",
    "        How to perform domain padding, by default 'one-sided'\n",
    "    fft_norm : str, optional\n",
    "        by default 'forward'\n",
    "    \"\"\"\n",
    "    def __init__(self, n_modes, hidden_channels,\n",
    "                 in_channels=3, \n",
    "                 out_channels=1,\n",
    "                 lifting_channels=256,\n",
    "                 projection_channels=256,\n",
    "                 n_layers=4,\n",
    "                 incremental_n_modes=None,\n",
    "                 use_mlp=False, mlp=None,\n",
    "                 non_linearity=F.gelu,\n",
    "                 norm=None, preactivation=False,\n",
    "                 fno_skip='linear',\n",
    "                 mlp_skip='soft-gating',\n",
    "                 separable=False,\n",
    "                 factorization=None,\n",
    "                 rank=1.0,\n",
    "                 joint_factorization=False, \n",
    "                 fixed_rank_modes=False,\n",
    "                 implementation='factorized',\n",
    "                 decomposition_kwargs=dict(),\n",
    "                 domain_padding=None,\n",
    "                 domain_padding_mode='one-sided',\n",
    "                 fft_norm='forward',\n",
    "                 SpectralConv=FactorizedSpectralConv,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_dim = len(n_modes)\n",
    "        self.n_modes = n_modes\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.lifting_channels = lifting_channels\n",
    "        self.projection_channels = projection_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_layers = n_layers\n",
    "        self.joint_factorization = joint_factorization\n",
    "        self.non_linearity = non_linearity\n",
    "        self.rank = rank\n",
    "        self.factorization = factorization\n",
    "        self.fixed_rank_modes = fixed_rank_modes\n",
    "        self.decomposition_kwargs = decomposition_kwargs\n",
    "        self.fno_skip = fno_skip,\n",
    "        self.mlp_skip = mlp_skip,\n",
    "        self.fft_norm = fft_norm\n",
    "        self.implementation = implementation\n",
    "        self.separable = separable\n",
    "        self.preactivation = preactivation\n",
    "\n",
    "        # See the class' property for underlying mechanism\n",
    "        # When updated, change should be reflected in fno blocks\n",
    "        self._incremental_n_modes = incremental_n_modes\n",
    "\n",
    "        if domain_padding is not None and domain_padding > 0:\n",
    "            self.domain_padding = DomainPadding(domain_padding=domain_padding, padding_mode=domain_padding_mode)\n",
    "        else:\n",
    "            self.domain_padding = None\n",
    "        self.domain_padding_mode = domain_padding_mode\n",
    "\n",
    "        self.fno_blocks = FNOBlocks(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels, \n",
    "            n_modes=self.n_modes,\n",
    "            use_mlp=use_mlp, mlp=mlp,\n",
    "            non_linearity=non_linearity,\n",
    "            norm=norm, preactivation=preactivation,\n",
    "            fno_skip=fno_skip,\n",
    "            mlp_skip=mlp_skip,\n",
    "            incremental_n_modes=incremental_n_modes,\n",
    "            rank=rank,\n",
    "            fft_norm=fft_norm,\n",
    "            fixed_rank_modes=fixed_rank_modes, \n",
    "            implementation=implementation,\n",
    "            separable=separable,\n",
    "            factorization=factorization,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            joint_factorization=joint_factorization,\n",
    "            SpectralConv=SpectralConv,\n",
    "            n_layers=n_layers)\n",
    "\n",
    "        self.lifting = Lifting(in_channels=in_channels, out_channels=self.hidden_channels, n_dim=self.n_dim)\n",
    "        self.projection = Projection(in_channels=self.hidden_channels, out_channels=out_channels, hidden_channels=projection_channels,\n",
    "                                     non_linearity=non_linearity, n_dim=self.n_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"TFNO's forward pass\n",
    "        \"\"\"\n",
    "        x = self.lifting(x)\n",
    "\n",
    "        if self.domain_padding is not None:\n",
    "            x = self.domain_padding.pad(x)\n",
    "\n",
    "        for layer_idx in range(self.n_layers):\n",
    "            x = self.fno_blocks(x, layer_idx)\n",
    "\n",
    "        if self.domain_padding is not None:\n",
    "            x = self.domain_padding.unpad(x)\n",
    "\n",
    "        x = self.projection(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def incremental_n_modes(self):\n",
    "        return self._incremental_n_modes\n",
    "\n",
    "    @incremental_n_modes.setter\n",
    "    def incremental_n_modes(self, incremental_n_modes):\n",
    "        self.fno_blocks.incremental_n_modes = incremental_n_modes\n",
    "\n",
    "\n",
    "class FNO1d(FNO):\n",
    "    \"\"\"1D Fourier Neural Operator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    modes_height : int\n",
    "        number of Fourier modes to keep along the height\n",
    "    hidden_channels : int\n",
    "        width of the FNO (i.e. number of channels)\n",
    "    in_channels : int, optional\n",
    "        Number of input channels, by default 3\n",
    "    out_channels : int, optional\n",
    "        Number of output channels, by default 1\n",
    "    lifting_channels : int, optional\n",
    "        number of hidden channels of the lifting block of the FNO, by default 256\n",
    "    projection_channels : int, optional\n",
    "        number of hidden channels of the projection block of the FNO, by default 256\n",
    "    n_layers : int, optional\n",
    "        Number of Fourier Layers, by default 4\n",
    "    incremental_n_modes : None or int tuple, default is None\n",
    "        * If not None, this allows to incrementally increase the number of modes in Fourier domain \n",
    "          during training. Has to verify n <= N for (n, m) in zip(incremental_n_modes, n_modes).\n",
    "        \n",
    "        * If None, all the n_modes are used.\n",
    "\n",
    "        This can be updated dynamically during training.\n",
    "    use_mlp : bool, optional\n",
    "        Whether to use an MLP layer after each FNO block, by default False\n",
    "    mlp : dict, optional\n",
    "        Parameters of the MLP, by default None\n",
    "        {'expansion': float, 'dropout': float}\n",
    "    non_linearity : nn.Module, optional\n",
    "        Non-Linearity module to use, by default F.gelu\n",
    "    norm : F.module, optional\n",
    "        Normalization layer to use, by default None\n",
    "    preactivation : bool, default is False\n",
    "        if True, use resnet-style preactivation\n",
    "    skip : {'linear', 'identity', 'soft-gating'}, optional\n",
    "        Type of skip connection to use, by default 'soft-gating'\n",
    "    separable : bool, default is False\n",
    "        if True, use a depthwise separable spectral convolution\n",
    "    factorization : str or None, {'tucker', 'cp', 'tt'}\n",
    "        Tensor factorization of the parameters weight to use, by default None.\n",
    "        * If None, a dense tensor parametrizes the Spectral convolutions\n",
    "        * Otherwise, the specified tensor factorization is used.\n",
    "    joint_factorization : bool, optional\n",
    "        Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer), by default False\n",
    "    rank : float or rank, optional\n",
    "        Rank of the tensor factorization of the Fourier weights, by default 1.0\n",
    "    fixed_rank_modes : bool, optional\n",
    "        Modes to not factorize, by default False\n",
    "    implementation : {'factorized', 'reconstructed'}, optional, default is 'factorized'\n",
    "        If factorization is not None, forward mode to use::\n",
    "        * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass\n",
    "        * `factorized` : the input is directly contracted with the factors of the decomposition\n",
    "    decomposition_kwargs : dict, optional, default is {}\n",
    "        Optionaly additional parameters to pass to the tensor decomposition\n",
    "    domain_padding : None or float, optional\n",
    "        If not None, percentage of padding to use, by default None\n",
    "    domain_padding_mode : {'symmetric', 'one-sided'}, optional\n",
    "        How to perform domain padding, by default 'one-sided'\n",
    "    fft_norm : str, optional\n",
    "        by default 'forward'\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_modes_height,\n",
    "        hidden_channels,\n",
    "        in_channels=3, \n",
    "        out_channels=1,\n",
    "        lifting_channels=256,\n",
    "        projection_channels=256,\n",
    "        incremental_n_modes=None,\n",
    "        n_layers=4,\n",
    "        non_linearity=F.gelu,\n",
    "        use_mlp=False, mlp=None,\n",
    "        norm=None,\n",
    "        skip='soft-gating',\n",
    "        separable=False,\n",
    "        preactivation=False,\n",
    "        factorization=None, \n",
    "        rank=1.0,\n",
    "        joint_factorization=False, \n",
    "        fixed_rank_modes=False,\n",
    "        implementation='factorized',\n",
    "        decomposition_kwargs=dict(),\n",
    "        domain_padding=None,\n",
    "        domain_padding_mode='one-sided',\n",
    "        fft_norm='forward',\n",
    "        **kwargs):\n",
    "        super().__init__(\n",
    "            n_modes=(n_modes_height, ),\n",
    "            hidden_channels=hidden_channels,\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            lifting_channels=lifting_channels,\n",
    "            projection_channels=projection_channels,\n",
    "            n_layers=n_layers,\n",
    "            non_linearity=non_linearity,\n",
    "            use_mlp=use_mlp, mlp=mlp,\n",
    "            incremental_n_modes=incremental_n_modes,\n",
    "            norm=norm,\n",
    "            skip=skip,\n",
    "            separable=separable,\n",
    "            preactivation=preactivation,\n",
    "            factorization=factorization, \n",
    "            rank=rank,\n",
    "            joint_factorization=joint_factorization, \n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            domain_padding=domain_padding,\n",
    "            domain_padding_mode=domain_padding_mode,\n",
    "            fft_norm=fft_norm\n",
    "            )\n",
    "        self.n_modes_height = n_modes_height\n",
    "\n",
    "\n",
    "class FNO2d(FNO):\n",
    "    \"\"\"2D Fourier Neural Operator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_modes_width : int\n",
    "        number of modes to keep in Fourier Layer, along the width\n",
    "    n_modes_height : int\n",
    "        number of Fourier modes to keep along the height\n",
    "    hidden_channels : int\n",
    "        width of the FNO (i.e. number of channels)\n",
    "    in_channels : int, optional\n",
    "        Number of input channels, by default 3\n",
    "    out_channels : int, optional\n",
    "        Number of output channels, by default 1\n",
    "    lifting_channels : int, optional\n",
    "        number of hidden channels of the lifting block of the FNO, by default 256\n",
    "    projection_channels : int, optional\n",
    "        number of hidden channels of the projection block of the FNO, by default 256\n",
    "    n_layers : int, optional\n",
    "        Number of Fourier Layers, by default 4\n",
    "    incremental_n_modes : None or int tuple, default is None\n",
    "        * If not None, this allows to incrementally increase the number of modes in Fourier domain \n",
    "          during training. Has to verify n <= N for (n, m) in zip(incremental_n_modes, n_modes).\n",
    "        \n",
    "        * If None, all the n_modes are used.\n",
    "\n",
    "        This can be updated dynamically during training.\n",
    "    use_mlp : bool, optional\n",
    "        Whether to use an MLP layer after each FNO block, by default False\n",
    "    mlp : dict, optional\n",
    "        Parameters of the MLP, by default None\n",
    "        {'expansion': float, 'dropout': float}\n",
    "    non_linearity : nn.Module, optional\n",
    "        Non-Linearity module to use, by default F.gelu\n",
    "    norm : F.module, optional\n",
    "        Normalization layer to use, by default None\n",
    "    preactivation : bool, default is False\n",
    "        if True, use resnet-style preactivation\n",
    "    skip : {'linear', 'identity', 'soft-gating'}, optional\n",
    "        Type of skip connection to use, by default 'soft-gating'\n",
    "    separable : bool, default is False\n",
    "        if True, use a depthwise separable spectral convolution\n",
    "    factorization : str or None, {'tucker', 'cp', 'tt'}\n",
    "        Tensor factorization of the parameters weight to use, by default None.\n",
    "        * If None, a dense tensor parametrizes the Spectral convolutions\n",
    "        * Otherwise, the specified tensor factorization is used.\n",
    "    joint_factorization : bool, optional\n",
    "        Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer), by default False\n",
    "    rank : float or rank, optional\n",
    "        Rank of the tensor factorization of the Fourier weights, by default 1.0\n",
    "    fixed_rank_modes : bool, optional\n",
    "        Modes to not factorize, by default False\n",
    "    implementation : {'factorized', 'reconstructed'}, optional, default is 'factorized'\n",
    "        If factorization is not None, forward mode to use::\n",
    "        * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass\n",
    "        * `factorized` : the input is directly contracted with the factors of the decomposition\n",
    "    decomposition_kwargs : dict, optional, default is {}\n",
    "        Optionaly additional parameters to pass to the tensor decomposition\n",
    "    domain_padding : None or float, optional\n",
    "        If not None, percentage of padding to use, by default None\n",
    "    domain_padding_mode : {'symmetric', 'one-sided'}, optional\n",
    "        How to perform domain padding, by default 'one-sided'\n",
    "    fft_norm : str, optional\n",
    "        by default 'forward'\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_modes_height,\n",
    "        n_modes_width,\n",
    "        hidden_channels,\n",
    "        in_channels=3, \n",
    "        out_channels=1,\n",
    "        lifting_channels=256,\n",
    "        projection_channels=256,\n",
    "        n_layers=4,\n",
    "        incremental_n_modes=None,\n",
    "        non_linearity=F.gelu,\n",
    "        use_mlp=False, mlp=None,\n",
    "        norm=None,\n",
    "        skip='soft-gating',\n",
    "        separable=False,\n",
    "        preactivation=False,\n",
    "        factorization=None, \n",
    "        rank=1.0,\n",
    "        joint_factorization=False, \n",
    "        fixed_rank_modes=False,\n",
    "        implementation='factorized',\n",
    "        decomposition_kwargs=dict(),\n",
    "        domain_padding=None,\n",
    "        domain_padding_mode='one-sided',\n",
    "        fft_norm='forward',\n",
    "        **kwargs):\n",
    "        super().__init__(\n",
    "            n_modes=(n_modes_height, n_modes_width),\n",
    "            hidden_channels=hidden_channels,\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            lifting_channels=lifting_channels,\n",
    "            projection_channels=projection_channels,\n",
    "            n_layers=n_layers,\n",
    "            non_linearity=non_linearity,\n",
    "            use_mlp=use_mlp, mlp=mlp,\n",
    "            incremental_n_modes=incremental_n_modes,\n",
    "            norm=norm,\n",
    "            skip=skip,\n",
    "            separable=separable,\n",
    "            preactivation=preactivation,\n",
    "            factorization=factorization, \n",
    "            rank=rank,\n",
    "            joint_factorization=joint_factorization, \n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            domain_padding=domain_padding,\n",
    "            domain_padding_mode=domain_padding_mode,\n",
    "            fft_norm=fft_norm\n",
    "            )\n",
    "        self.n_modes_height = n_modes_height\n",
    "        self.n_modes_width = n_modes_width\n",
    "\n",
    "\n",
    "\n",
    "class FNO3d(FNO):\n",
    "    \"\"\"3D Fourier Neural Operator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    modes_width : int\n",
    "        number of modes to keep in Fourier Layer, along the width\n",
    "    modes_height : int\n",
    "        number of Fourier modes to keep along the height    \n",
    "    modes_depth : int\n",
    "        number of Fourier modes to keep along the depth\n",
    "    hidden_channels : int\n",
    "        width of the FNO (i.e. number of channels)\n",
    "    in_channels : int, optional\n",
    "        Number of input channels, by default 3\n",
    "    out_channels : int, optional\n",
    "        Number of output channels, by default 1\n",
    "    lifting_channels : int, optional\n",
    "        number of hidden channels of the lifting block of the FNO, by default 256\n",
    "    projection_channels : int, optional\n",
    "        number of hidden channels of the projection block of the FNO, by default 256\n",
    "    n_layers : int, optional\n",
    "        Number of Fourier Layers, by default 4\n",
    "    incremental_n_modes : None or int tuple, default is None\n",
    "        * If not None, this allows to incrementally increase the number of modes in Fourier domain \n",
    "          during training. Has to verify n <= N for (n, m) in zip(incremental_n_modes, n_modes).\n",
    "        \n",
    "        * If None, all the n_modes are used.\n",
    "\n",
    "        This can be updated dynamically during training.\n",
    "    use_mlp : bool, optional\n",
    "        Whether to use an MLP layer after each FNO block, by default False\n",
    "    mlp : dict, optional\n",
    "        Parameters of the MLP, by default None\n",
    "        {'expansion': float, 'dropout': float}\n",
    "    non_linearity : nn.Module, optional\n",
    "        Non-Linearity module to use, by default F.gelu\n",
    "    norm : F.module, optional\n",
    "        Normalization layer to use, by default None\n",
    "    preactivation : bool, default is False\n",
    "        if True, use resnet-style preactivation\n",
    "    skip : {'linear', 'identity', 'soft-gating'}, optional\n",
    "        Type of skip connection to use, by default 'soft-gating'\n",
    "    separable : bool, default is False\n",
    "        if True, use a depthwise separable spectral convolution\n",
    "    factorization : str or None, {'tucker', 'cp', 'tt'}\n",
    "        Tensor factorization of the parameters weight to use, by default None.\n",
    "        * If None, a dense tensor parametrizes the Spectral convolutions\n",
    "        * Otherwise, the specified tensor factorization is used.\n",
    "    joint_factorization : bool, optional\n",
    "        Whether all the Fourier Layers should be parametrized by a single tensor (vs one per layer), by default False\n",
    "    rank : float or rank, optional\n",
    "        Rank of the tensor factorization of the Fourier weights, by default 1.0\n",
    "    fixed_rank_modes : bool, optional\n",
    "        Modes to not factorize, by default False\n",
    "    implementation : {'factorized', 'reconstructed'}, optional, default is 'factorized'\n",
    "        If factorization is not None, forward mode to use::\n",
    "        * `reconstructed` : the full weight tensor is reconstructed from the factorization and used for the forward pass\n",
    "        * `factorized` : the input is directly contracted with the factors of the decomposition\n",
    "    decomposition_kwargs : dict, optional, default is {}\n",
    "        Optionaly additional parameters to pass to the tensor decomposition\n",
    "    domain_padding : None or float, optional\n",
    "        If not None, percentage of padding to use, by default None\n",
    "    domain_padding_mode : {'symmetric', 'one-sided'}, optional\n",
    "        How to perform domain padding, by default 'one-sided'\n",
    "    fft_norm : str, optional\n",
    "        by default 'forward'\n",
    "    \"\"\"\n",
    "    def __init__(self,                  \n",
    "        n_modes_height,\n",
    "        n_modes_width,\n",
    "        n_modes_depth,\n",
    "        hidden_channels,\n",
    "        in_channels=3, \n",
    "        out_channels=1,\n",
    "        lifting_channels=256,\n",
    "        projection_channels=256,\n",
    "        n_layers=4,\n",
    "        incremental_n_modes=None,\n",
    "        non_linearity=F.gelu,\n",
    "        use_mlp=False, mlp=None,\n",
    "        norm=None,\n",
    "        skip='soft-gating',\n",
    "        separable=False,\n",
    "        preactivation=False,\n",
    "        factorization=None, \n",
    "        rank=1.0,\n",
    "        joint_factorization=False, \n",
    "        fixed_rank_modes=False,\n",
    "        implementation='factorized',\n",
    "        decomposition_kwargs=dict(),\n",
    "        domain_padding=None,\n",
    "        domain_padding_mode='one-sided',\n",
    "        fft_norm='forward',\n",
    "        **kwargs):\n",
    "        super().__init__(\n",
    "            n_modes=(n_modes_height, n_modes_width, n_modes_depth),\n",
    "            hidden_channels=hidden_channels,\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels,\n",
    "            lifting_channels=lifting_channels,\n",
    "            projection_channels=projection_channels,\n",
    "            n_layers=n_layers,\n",
    "            non_linearity=non_linearity,\n",
    "            incremental_n_modes=incremental_n_modes,\n",
    "            use_mlp=use_mlp, mlp=mlp,\n",
    "            norm=norm,\n",
    "            skip=skip,\n",
    "            separable=separable,\n",
    "            preactivation=preactivation,\n",
    "            factorization=factorization, \n",
    "            rank=rank,\n",
    "            joint_factorization=joint_factorization, \n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation,\n",
    "            decomposition_kwargs=decomposition_kwargs,\n",
    "            domain_padding=domain_padding,\n",
    "            domain_padding_mode=domain_padding_mode,\n",
    "            fft_norm=fft_norm\n",
    "            )\n",
    "        self.n_modes_height = n_modes_height\n",
    "        self.n_modes_width = n_modes_width\n",
    "        self.n_modes_height = n_modes_height\n",
    "\n",
    "\n",
    "def partialclass(new_name, cls, *args, **kwargs):\n",
    "    \"\"\"Create a new class with different default values\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    An obvious alternative would be to use functools.partial\n",
    "    >>> new_class = partial(cls, **kwargs)\n",
    "\n",
    "    The issue is twofold:\n",
    "    1. the class doesn't have a name, so one would have to set it explicitly:\n",
    "    >>> new_class.__name__ = new_name\n",
    "\n",
    "    2. the new class will be a functools object and one cannot inherit from it.\n",
    "\n",
    "    Instead, here, we define dynamically a new class, inheriting from the existing one.\n",
    "    \"\"\"\n",
    "    __init__ = partialmethod(cls.__init__, *args, **kwargs)\n",
    "    new_class = type(new_name, (cls,),  {\n",
    "        '__init__': __init__,\n",
    "        '__doc__': cls.__doc__,\n",
    "        'forward': cls.forward, \n",
    "    })\n",
    "    return new_class\n",
    "\n",
    "\n",
    "TFNO   = partialclass('TFNO', FNO, factorization='Tucker')\n",
    "TFNO1d = partialclass('TFNO1d', FNO1d, factorization='Tucker')\n",
    "TFNO2d = partialclass('TFNO2d', FNO2d, factorization='Tucker')\n",
    "TFNO3d = partialclass('TFNO3d', FNO3d, factorization='Tucker')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavebench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
